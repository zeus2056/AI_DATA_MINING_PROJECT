# -*- coding: utf-8 -*-
"""Untitled8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Pkb-w02qF0aEiVtXw-3OxdwoFbt-cPs1
"""

import pandas as pd
from sklearn.utils import shuffle


twitter_df = pd.read_csv("/content/drive/MyDrive/Dataset/Twitter_Data_Updated.csv")
reddit_df = pd.read_csv("/content/drive/MyDrive/Dataset/Redit_Data_Updated.csv")
test_df = pd.read_csv("/content/drive/MyDrive/Dataset/test.csv")

for df in [twitter_df, reddit_df, test_df]:
    if "label" in df.columns:
        df.rename(columns={"label": "category"}, inplace=True)
    if "text" in df.columns:
        df.rename(columns={"text": "text"}, inplace=True)


df = pd.concat([twitter_df, reddit_df, test_df], ignore_index=True)


df = df.dropna(subset=["text", "category"])


df["category"] = df["category"].replace({"Notr":"Neutral","notr":"Neutral","NOTR":"Neutral"})
label2id = {"Negative":0, "Neutral":1, "Positive":2}
df["label"] = df["category"].map(label2id)

min_count = df["label"].value_counts().min()
print("SÄ±nÄ±f baÅŸÄ±na kullanÄ±labilecek maksimum Ã¶rnek sayÄ±sÄ±:", min_count)

df_balanced = pd.concat([
    df[df["label"]==0].sample(min_count, random_state=42),
    df[df["label"]==1].sample(min_count, random_state=42),
    df[df["label"]==2].sample(min_count, random_state=42)
])


df_balanced = shuffle(df_balanced, random_state=42).reset_index(drop=True)

train_size = int(0.6 * len(df_balanced))
val_size = int(0.2 * len(df_balanced))

train_df = df_balanced.iloc[:train_size]
val_df = df_balanced.iloc[train_size:train_size+val_size]
test_df = df_balanced.iloc[train_size+val_size:]

train_df.to_csv("/content/drive/MyDrive/Dataset/train_balanced_max.csv", index=False)
val_df.to_csv("/content/drive/MyDrive/Dataset/val_balanced_max.csv", index=False)
test_df.to_csv("/content/drive/MyDrive/Dataset/test_balanced_max.csv", index=False)

print("âœ… Her sette etiket daÄŸÄ±lÄ±mÄ± (maksimum veri ile dengeli)")
print("\nTrain:\n", train_df["category"].value_counts())
print("\nValidation:\n", val_df["category"].value_counts())
print("\nTest:\n", test_df["category"].value_counts())

train_df.to_csv("/content/drive/MyDrive/Dataset/train_balanced_max.csv", index=False)
val_df.to_csv("/content/drive/MyDrive/Dataset/val_balanced_max.csv", index=False)
test_df.to_csv("/content/drive/MyDrive/Dataset/test_balanced_max.csv", index=False)

print("âœ… Veriler Drive'a kaydedildi:")
print("/content/drive/MyDrive/Dataset/train_balanced_max.csv")
print("/content/drive/MyDrive/Dataset/val_balanced_max.csv")
print("/content/drive/MyDrive/Dataset/test_balanced_max.csv")

!pip install transformers datasets torch --quiet

import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from torch.optim import AdamW

from sklearn.metrics import classification_report, confusion_matrix

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", device)

train_df = pd.read_csv("/content/drive/MyDrive/Dataset/train_balanced_max.csv")
val_df = pd.read_csv("/content/drive/MyDrive/Dataset/val_balanced_max.csv")
test_df = pd.read_csv("/content/drive/MyDrive/Dataset/test_balanced_max.csv")

class SentimentDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len=128):
        self.texts = list(texts)
        self.labels = list(labels)
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        encoding = self.tokenizer(
            self.texts[idx],
            truncation=True,
            padding='max_length',
            max_length=self.max_len,
            return_tensors='pt'
        )
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(self.labels[idx], dtype=torch.long)
        }

MODEL_NAME = "xlm-roberta-base"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=3)
model.to(device)


train_dataset = SentimentDataset(train_df["text"], train_df["label"], tokenizer)
val_dataset = SentimentDataset(val_df["text"], val_df["label"], tokenizer)
test_dataset = SentimentDataset(test_df["text"], test_df["label"], tokenizer)

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=16)
test_loader = DataLoader(test_dataset, batch_size=16)


import numpy as np
classes, counts = np.unique(train_df["label"], return_counts=True)
total = len(train_df)
weights = [total/(len(classes)*c) for c in counts]
class_weights = torch.tensor(weights, dtype=torch.float).to(device)

loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)
optimizer = AdamW(model.parameters(), lr=5e-5)


epochs = 3
for epoch in range(epochs):
    model.train()
    total_loss = 0
    for batch in train_loader:
        optimizer.zero_grad()
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)

        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        loss = loss_fn(logits, labels)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
    print(f"Epoch {epoch+1}/{epochs} | Loss: {total_loss/len(train_loader):.4f}")


model.eval()
all_preds = []
all_labels = []

with torch.no_grad():
    for batch in test_loader:
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        preds = torch.argmax(outputs.logits, dim=1)
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

print("\nðŸ§¾ Classification Report:")
print(classification_report(all_labels, all_preds, target_names=["Negative","Neutral","Positive"]))

print("\nðŸ“Š Confusion Matrix:")
print(confusion_matrix(all_labels, all_preds))

model_save_path = "/content/drive/MyDrive/Dataset/xlm_roberta_sentiment_model"
model.save_pretrained(model_save_path)
tokenizer.save_pretrained(model_save_path)
print(f"\nâœ… Model baÅŸarÄ±yla kaydedildi: {model_save_path}")

!pip install gradio transformers --quiet
import gradio as gr
import torch
from transformers import AutoModelForSequenceClassification, AutoTokenizer


model_path = "/content/drive/MyDrive/xlm_roberta_sentiment_model"
model = AutoModelForSequenceClassification.from_pretrained(model_path)
tokenizer = AutoTokenizer.from_pretrained(model_path)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)
model.eval()


id2label = {0:"Negative", 1:"Neutral", 2:"Positive"}


def predict_sentiment(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True).to(device)
    outputs = model(**inputs)
    pred = torch.argmax(outputs.logits, dim=1).item()
    return id2label[pred]

demo = gr.Interface(
    fn=predict_sentiment,
    inputs=gr.Textbox(lines=2, placeholder="Bir metin girin..."),
    outputs="text",
    title="Sentiment Analysis (Roberta)",
    description="Bu model Roberta kullanarak metni Pozitif, Negatif veya NÃ¶tr olarak sÄ±nÄ±flandÄ±rÄ±r."
)

demo.launch(share=True)